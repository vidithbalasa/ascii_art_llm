{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ptYY_yh6PZC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"ascii_art_data.json\")\n"
      ],
      "metadata": {
        "id": "yeyD4J6u6lJM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['text'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0G4Z_036wDL",
        "outputId": "a8da34d2-2244-4099-c6a7-f92fe1aa1cd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       _.---._    /\\\\\r\n",
            "    ./'       \"--`\\//\r\n",
            "  ./              o \\          .-----.\r\n",
            " /./\\  )______   \\__ \\        ( help! )\r\n",
            "./  / /\\ \\   | \\ \\  \\ \\       /`-----'\r\n",
            "   / /  \\ \\  | |\\ \\  \\7--- ooo ooo ooo ooo ooo ooo\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a variable called ASCII_TEXT which is a list of all the df['text'] items where the width is less than 32 AND the height is less than 32. You might have to first calculate the width and height for each item before filtering since they're just strings. Width in this case is the width of the longest line, not the average width of the ascii image\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"ascii_art_data.json\")\n",
        "\n",
        "df['width'] = df['text'].apply(lambda x: max(len(line) for line in x.splitlines()))\n",
        "df['height'] = df['text'].apply(lambda x: len(x.splitlines()))\n",
        "\n",
        "ASCII_TEXT = df[ (df['width'] < 32) & (df['height'] < 32) ]['text'].tolist()\n",
        "len(ASCII_TEXT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J4WLyVx8yaF",
        "outputId": "a90c3f96-8c3e-45e1-f9f0-12e5feca8107"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2108"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ASCIITokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = [chr(i) for i in range(32, 127)]  # printable ASCII characters\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.token_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
        "        self.char_to_token = {c: i for i, c in enumerate(self.vocab)}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = [self.char_to_token[c] for c in text if c in self.vocab]\n",
        "        return tokens\n",
        "\n",
        "tokenizer = ASCIITokenizer()\n",
        "\n",
        "tokenized_data = []\n",
        "for ascii_art in ASCII_TEXT:\n",
        "    tokens = tokenizer.tokenize(ascii_art)\n",
        "    tokenized_data.append(tokens)\n",
        "\n",
        "print(f'Num Unique Tokens: {len(tokenizer.token_to_char)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQvGx116592",
        "outputId": "2cf3368a-5ccb-43e6-c83e-af6007fa191f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Unique Tokens: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing to note is that the tokenization here tokenizes each character individually, meaning that `\\` and `n` are individual tokens, I think it'll be fine and the model will learn that they should be combined for a new line but I don't know if we have enough data and time for it to properly learn that."
      ],
      "metadata": {
        "id": "l3JiBLYE_ERB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the padding I'm doing underneath is definitely an area we gotta do some more experimentation on. Right now it's just flattening the art into a long string and padding it at the end but it might be better to add padding in between to center the image. Not sure."
      ],
      "metadata": {
        "id": "jiAY6ary-oNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_ascii_art(ascii_art, max_width=32, max_height=32, pad_char=' '):\n",
        "    lines = ascii_art.splitlines()\n",
        "    padded_lines = [line.ljust(max_width, pad_char) for line in lines]\n",
        "    while len(padded_lines) < max_height:\n",
        "        padded_lines.append(pad_char * max_width)\n",
        "    return '\\n'.join(padded_lines)\n",
        "\n",
        "padded_data = [pad_ascii_art(art) for art in ASCII_TEXT]\n",
        "\n",
        "# Tokenize the padded data\n",
        "tokenized_data = [tokenizer.tokenize(art) for art in padded_data]\n",
        "\n",
        "# Convert to tensor and pad to 32x32 (1024 tokens)\n",
        "max_length = 32 * 32\n",
        "tokenized_data = [tokens + [0] * (max_length - len(tokens)) for tokens in tokenized_data]\n",
        "tokenized_data = torch.tensor(tokenized_data)"
      ],
      "metadata": {
        "id": "Ikw8B6SR-ofs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "dataset = TensorDataset(tokenized_data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "_BnFyQDuAt6a"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ],
      "metadata": {
        "id": "CwfVtw1eyMQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim * 2)  # for mean and log variance\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()  # output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h.chunk(2, dim=-1)\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        # Decode\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        return x_reconstructed, mu, logvar"
      ],
      "metadata": {
        "id": "VaBr46P6yq9O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "XWlYUNUoytUs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_dim = 32 * 32  # 1024\n",
        "hidden_dim = 512\n",
        "latent_dim = 64\n",
        "lr = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "model = VAE(input_dim, hidden_dim, latent_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "8oZiPzfH0ah0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 95  # Number of printable ASCII characters\n",
        "tokenized_data = tokenized_data.float() / vocab_size"
      ],
      "metadata": {
        "id": "_XaeG36V03Wk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = batch[0].float() / vocab_size\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(batch)\n",
        "        loss = loss_function(recon_batch, batch, mu, logvar)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader.dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik8lZZBy0dS8",
        "outputId": "de84ddcc-a638-4eff-a128-4d463fbdf561"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 210.8883853456327\n",
            "Epoch 2, Loss: 137.3592195746116\n",
            "Epoch 3, Loss: 133.83079514331564\n",
            "Epoch 4, Loss: 129.1705778581594\n",
            "Epoch 5, Loss: 126.31899068930117\n",
            "Epoch 6, Loss: 123.30519025827947\n",
            "Epoch 7, Loss: 121.30071662454044\n",
            "Epoch 8, Loss: 119.97072536171727\n",
            "Epoch 9, Loss: 118.78508571791242\n",
            "Epoch 10, Loss: 118.2915747857863\n",
            "Epoch 11, Loss: 117.82751997598434\n",
            "Epoch 12, Loss: 117.19477418613614\n",
            "Epoch 13, Loss: 116.37714132638312\n",
            "Epoch 14, Loss: 116.17135266877668\n",
            "Epoch 15, Loss: 115.46657751904945\n",
            "Epoch 16, Loss: 115.09399842582572\n",
            "Epoch 17, Loss: 114.74855415011267\n",
            "Epoch 18, Loss: 114.07342262919532\n",
            "Epoch 19, Loss: 113.61534579021881\n",
            "Epoch 20, Loss: 112.96608414188508\n",
            "Epoch 21, Loss: 112.65668685884133\n",
            "Epoch 22, Loss: 112.09649855090726\n",
            "Epoch 23, Loss: 111.51719813193044\n",
            "Epoch 24, Loss: 111.36693710529826\n",
            "Epoch 25, Loss: 111.00548482529352\n",
            "Epoch 26, Loss: 110.42333463201939\n",
            "Epoch 27, Loss: 110.00803058197195\n",
            "Epoch 28, Loss: 109.86490337219817\n",
            "Epoch 29, Loss: 109.5405399677197\n",
            "Epoch 30, Loss: 109.11557510636563\n",
            "Epoch 31, Loss: 108.95209122427953\n",
            "Epoch 32, Loss: 108.64241737206476\n",
            "Epoch 33, Loss: 108.48626500515151\n",
            "Epoch 34, Loss: 108.04810311536409\n",
            "Epoch 35, Loss: 107.62145752879655\n",
            "Epoch 36, Loss: 107.31305475976933\n",
            "Epoch 37, Loss: 106.9960075827206\n",
            "Epoch 38, Loss: 106.74321282113526\n",
            "Epoch 39, Loss: 106.7622008929895\n",
            "Epoch 40, Loss: 106.33791853671282\n",
            "Epoch 41, Loss: 106.06561256133628\n",
            "Epoch 42, Loss: 105.94159966311373\n",
            "Epoch 43, Loss: 105.8508520832098\n",
            "Epoch 44, Loss: 105.71110276976844\n",
            "Epoch 45, Loss: 105.10690116520398\n",
            "Epoch 46, Loss: 104.96458490071306\n",
            "Epoch 47, Loss: 104.83251779400646\n",
            "Epoch 48, Loss: 104.66557471259043\n",
            "Epoch 49, Loss: 104.4595422618077\n",
            "Epoch 50, Loss: 104.32923266647889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_ascii_art(model, num_images):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Sample random latent vectors\n",
        "        z = torch.randn(num_images, model.decoder[0].in_features).float()\n",
        "        # Generate images by passing the latent vectors through the decoder\n",
        "        generated_images = model.decoder(z)\n",
        "\n",
        "    # Denormalize the generated images (values from 0 to 1 back to 0 to 94)\n",
        "    generated_images = (generated_images * vocab_size).int().clamp(0, vocab_size - 1)\n",
        "\n",
        "    # Convert generated tokens back to ASCII characters\n",
        "    generated_ascii_art = []\n",
        "    for image in generated_images:\n",
        "        tokens = image.numpy().reshape(32, 32)  # Reshape to 32x32\n",
        "        ascii_art = '\\n'.join(''.join(tokenizer.token_to_char[token] for token in row) for row in tokens)\n",
        "        generated_ascii_art.append(ascii_art)\n",
        "\n",
        "    return generated_ascii_art\n",
        "\n",
        "# Generate 5 images\n",
        "NUM_IMAGES = 5\n",
        "generated_images = generate_ascii_art(model, NUM_IMAGES)\n",
        "\n",
        "# Print the generated ASCII art images\n",
        "for i, art in enumerate(generated_images):\n",
        "    print(f\"Generated ASCII Art {i + 1}:\\n{art}\\n{'-'*40}\")\n",
        "\n",
        "# Optional: Plot the generated ASCII art images using matplotlib (if desired)\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "for i, art in enumerate(generated_images):\n",
        "    axes[i].imshow([[ord(char) for char in line] for line in art.split('\\n')], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VdFk-3Ha43Wi",
        "outputId": "88633c6a-016f-43bd-b6b2-b28560d443e8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated ASCII Art 1:\n",
            "     !!\"##$&*/024-1'$##!        \n",
            "  !!\"$%)0+*/23;?GH09()('! \"     \n",
            "! \"\"$).).1-1//7>97>?-(('##\"     \n",
            "\"##\"'&+0-44:85831?<?93(&\"\"\"!    \n",
            "&*)#*.4;4459?79>HITJJL44)%&)    \n",
            "/13&,.;<=DOIQAJEHJXWXQM90+.(!   \n",
            "/'+$*.4.5157A:88<CE6=.+/($)&    \n",
            "'$&\"$%-1.0/7/06;9630401%%%-\"    \n",
            ",*+!$$*%*-0+-(3-(,*&$#\"\"#!\"!    \n",
            "!## ! !!!\"$$$\"\"\"!!\"!!!#!! #     \n",
            "        !!!\"\"!!!! !#!!!\"  #     \n",
            "          !!!\"\"\"\"! !!!!         \n",
            "           !! !                 \n",
            "                                \n",
            "           ! !     \"            \n",
            "              !!                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "----------------------------------------\n",
            "Generated ASCII Art 2:\n",
            "    !\"$%-+0588.+2,$\"!!!         \n",
            "  !\"$&*&10.54.C9F4.)!\"!\"        \n",
            " !#%)11/44?9;GKEFA<=+%$\"!!      \n",
            " #'&-0,.-=IIFKQEIUAI:0'\"\"       \n",
            "'):*6.3F?@M_^\\[ZbchZ[H2$$#!!    \n",
            "*4?,0026@>IAGROUQ][VU5>#$!\"     \n",
            "1IU9H>><MCFNT][[db]]R47-+%!     \n",
            "1BT1976DDCFMGJP@Q^SNH=A#(!\"     \n",
            "!)&%#&$*''.1/3-+&-$&$\"\" !       \n",
            " \"\"!!! \"#\"$%(+%$$$! ! \"         \n",
            " !!\"!!  \"!$##&+$%\"!\"!\"\"         \n",
            "          !  \"\"!\"!              \n",
            "            !    !              \n",
            "          ! !#  !               \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "----------------------------------------\n",
            "Generated ASCII Art 3:\n",
            "$&((&)/*/2:?:?73+'$\"\"           \n",
            "#%$#%$$\"#%%'&)'1%%*#\"!          \n",
            "! !$$#$%\"\"#$&%%&\"\"#!\"           \n",
            "\"\"!%$#$\"$&%%'%&$\"#$!\"           \n",
            "!!!$#\"##\"\"%%'%\"$#$%! !          \n",
            "!\" #!##\"\"$#$+$$+!*,\"            \n",
            "!! \"#%('$'%#$'\".\"#'$            \n",
            "\"   !\"##\"#\"!#!#%\"\"'!\"           \n",
            "(%#!\"'%)#.(/&,(2$'1)!!          \n",
            "('\"!'\"#(!%'()&$*#$#%!!          \n",
            "$!! !\"$&\"#\"$+%\"##%!!$\"!         \n",
            "('%!\"\"+#0*'#<&)((',#'\"# \"!      \n",
            "\"5\"!$ #%\"%\"$($!\"!!! !   !       \n",
            "%($\"&\"\"! !\"\"%!#!!!! !   #!      \n",
            "$)# !         !  !      !       \n",
            " !  \" !!     ! !                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "----------------------------------------\n",
            "Generated ASCII Art 4:\n",
            "  !!\"#(*324/2/*'(\"!             \n",
            " !#$&/0127270**,&$!!            \n",
            "!$&*,9A;@AKI@=66-(#&!!          \n",
            "&()).66;;9A;=92-1*#&$\"          \n",
            "/3804C:SKBOGA>43-()*'$! !       \n",
            "STT6AILNRU[RJKJ;4-)7)1($!       \n",
            ";94(+/10+-192/*-)$(\"\"!\"!!       \n",
            "%%$\"\"\"#%##\"&!\"!\"#!              \n",
            "!!  !!\"!!\"#$!!\"! !              \n",
            "      !!  \"!                    \n",
            "     !! \" !\"                    \n",
            "     !\" ! !! !                  \n",
            "     !\"  !!!                    \n",
            "      ! !!!                     \n",
            "      !! !\"!                    \n",
            "      ! \"!#! !                  \n",
            "  !  !#$#\"'\"!!!!     !          \n",
            "     #\"!\"!'\"!! \" !              \n",
            "      ! !!$!\"                   \n",
            "        !!                      \n",
            "       !                        \n",
            "        !!                      \n",
            "      ! ! ! !                   \n",
            "        !                       \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "                                \n",
            "----------------------------------------\n",
            "Generated ASCII Art 5:\n",
            "       !\"\"%%)(%#! !!            \n",
            "     !!\"$##,%()'\"# !!!          \n",
            "   !!!##$$&-13,,)(##$\"\"$!!  !   \n",
            "  !!!\"##,(,.20+**+##\"#$&\"$      \n",
            "  !!!\"#$''1*1.'(#0(%###'!\"!     \n",
            "  \" !%#%$)'.92.+&$&(&##*#\"$!    \n",
            "!%###*0,'/63?99/*)-+($&/($&$!!  \n",
            "!&&#'$,+,(*00720)'+*()*/-&!!$!  \n",
            "#)&\"',,4&3<5.F9.,03/21.0+$\"$'   \n",
            "\"#)'*1,0.7,/0?486>1021+.)+)#.$  \n",
            "#\",%&71?.2,63@45240624.).%!#\"!  \n",
            "),,$-.>224493G00<1433A131-#$!*  \n",
            ",C9).@D@897</>/4*-.C8,,2:/!\"!!  \n",
            "-A3+)::=4<:@=J@C93>:--+7/'!\"    \n",
            "';3(.-12+).9*>;/01':#%3:*'##\"$  \n",
            "&:-)1=4)*.+A1>85-)(/).'413('!$  \n",
            "&=:&,5361;5M5;4=.:7-&-+(*+.:*%  \n",
            "7_G+*8H=;D<V=46@*3C?;51+%)3*,%  \n",
            "4QJ/18B;8<IJAV=S0B8:9/*5''):7/  \n",
            "*@3!#57/.*EJAP09-@1>;.\"$$)!#'#  \n",
            " \"(#$\"%%&#&.(0&#\"%$\"$)!!\"!\"!    \n",
            " !# !\"#\"#\"-+\"!!$ !\"!\"( !\" !! \"  \n",
            "   !  !   !\" !   !  !$          \n",
            "     !    \"\" !  ! !! !   !      \n",
            "      \"   !\"   !  !             \n",
            "   !  !!!  ! !                  \n",
            "               !      !         \n",
            "                 !              \n",
            "       !        !               \n",
            "                      !         \n",
            "     ! !         !              \n",
            "                                \n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
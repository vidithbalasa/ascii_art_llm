{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ptYY_yh6PZC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"ascii_art_data.json\")\n"
      ],
      "metadata": {
        "id": "yeyD4J6u6lJM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['text'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0G4Z_036wDL",
        "outputId": "2cf798aa-aeae-40ea-b7cb-2a3b9c5edba1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       _.---._    /\\\\\r\n",
            "    ./'       \"--`\\//\r\n",
            "  ./              o \\          .-----.\r\n",
            " /./\\  )______   \\__ \\        ( help! )\r\n",
            "./  / /\\ \\   | \\ \\  \\ \\       /`-----'\r\n",
            "   / /  \\ \\  | |\\ \\  \\7--- ooo ooo ooo ooo ooo ooo\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a variable called ASCII_TEXT which is a list of all the df['text'] items where the width is less than 32 AND the height is less than 32. You might have to first calculate the width and height for each item before filtering since they're just strings. Width in this case is the width of the longest line, not the average width of the ascii image\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"ascii_art_data.json\")\n",
        "\n",
        "df['width'] = df['text'].apply(lambda x: max(len(line) for line in x.splitlines()))\n",
        "df['height'] = df['text'].apply(lambda x: len(x.splitlines()))\n",
        "\n",
        "ASCII_TEXT = df[ (df['width'] < 32) & (df['height'] < 32) ]['text'].tolist()\n",
        "len(ASCII_TEXT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J4WLyVx8yaF",
        "outputId": "8bac266d-29ec-4458-c35f-0a0f8f6f0552"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2108"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ASCIITokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = [chr(i) for i in range(32, 127)] + ['<PAD>']\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.token_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
        "        self.char_to_token = {c: i for i, c in enumerate(self.vocab)}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = [self.char_to_token[c] for c in text if c in self.vocab]\n",
        "        return tokens\n",
        "\n",
        "tokenizer = ASCIITokenizer()\n",
        "\n",
        "tokenized_data = []\n",
        "for ascii_art in ASCII_TEXT:\n",
        "    tokens = tokenizer.tokenize(ascii_art)\n",
        "    tokenized_data.append(tokens)\n",
        "\n",
        "print(f'Num Unique Tokens: {len(tokenizer.token_to_char)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQvGx116592",
        "outputId": "3faf9908-640a-4cda-b56b-da415624d07f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Unique Tokens: 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.char_to_token['\\\\']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh4dsFwu4ly9",
        "outputId": "5dd0d40a-2c0b-4286-96aa-294232166c45"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing to note is that the tokenization here tokenizes each character individually, meaning that `\\` and `n` are individual tokens, I think it'll be fine and the model will learn that they should be combined for a new line but I don't know if we have enough data and time for it to properly learn that."
      ],
      "metadata": {
        "id": "l3JiBLYE_ERB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the padding I'm doing underneath is definitely an area we gotta do some more experimentation on. Right now it's just flattening the art into a long string and padding it at the end but it might be better to add padding in between to center the image. Not sure."
      ],
      "metadata": {
        "id": "jiAY6ary-oNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_ascii_art(ascii_art, max_width=32, max_height=32, pad_char='<PAD>'):\n",
        "    lines = ascii_art.splitlines()\n",
        "    padded_lines = [line.ljust(max_width, pad_char) for line in lines]\n",
        "    while len(padded_lines) < max_height:\n",
        "        padded_lines.append(pad_char * max_width)\n",
        "    return '\\n'.join(padded_lines)\n",
        "\n",
        "padded_data = [pad_ascii_art(art) for art in ASCII_TEXT]\n",
        "\n",
        "# Tokenize the padded data\n",
        "tokenized_data = [tokenizer.tokenize(art) for art in padded_data]\n",
        "\n",
        "# Convert to tensor and pad to 32x32 (1024 tokens)\n",
        "max_length = 32 * 32\n",
        "tokenized_data = [tokens + [0] * (max_length - len(tokens)) for tokens in tokenized_data]\n",
        "tokenized_data = torch.tensor(tokenized_data)"
      ],
      "metadata": {
        "id": "Ikw8B6SR-ofs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "8f63d04e-60a1-47f2-da85-19d34c23c6e4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "The fill character must be exactly one character long",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-aa472d64e809>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpadded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_ascii_art\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mASCII_TEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Tokenize the padded data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-aa472d64e809>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpadded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_ascii_art\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mASCII_TEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Tokenize the padded data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-aa472d64e809>\u001b[0m in \u001b[0;36mpad_ascii_art\u001b[0;34m(ascii_art, max_width, max_height, pad_char)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpad_ascii_art\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascii_art\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_char\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mascii_art\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpadded_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_char\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_lines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpadded_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_char\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-aa472d64e809>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpad_ascii_art\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascii_art\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_char\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mascii_art\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpadded_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_char\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_lines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpadded_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_char\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The fill character must be exactly one character long"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "dataset = TensorDataset(tokenized_data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "_BnFyQDuAt6a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "NE5ZRdDwtVgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rqv1L0-taCh",
        "outputId": "b1a3acd4-a6cc-4557-c893-0cdb5c5e0779"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ASCIITransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, output_dim):\n",
        "        super(ASCIITransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(1, 0, 2)  # batch_size, seq_len, embedding_dim\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # batch_size, seq_len, embedding_dim\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = ASCIITransformer(vocab_size=len(tokenizer.token_to_char),\n",
        "                          embedding_dim=128,\n",
        "                          num_heads=4,\n",
        "                          hidden_dim=256,\n",
        "                          output_dim=len(tokenizer.token_to_char)\n",
        "        ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs.view(-1, len(tokenizer.token_to_char)), input_ids.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su7j_UFdtWzp",
        "outputId": "21acbfe5-103d-4b17-ebf1-49d200eed82b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.03230954334139824\n",
            "Epoch 2, Loss: 0.023365095257759094\n",
            "Epoch 3, Loss: 0.008311551995575428\n",
            "Epoch 4, Loss: 0.005041135940700769\n",
            "Epoch 5, Loss: 0.002155726309865713\n",
            "Epoch 6, Loss: 0.0019831599202007055\n",
            "Epoch 7, Loss: 0.0011665376368910074\n",
            "Epoch 8, Loss: 0.001104850904084742\n",
            "Epoch 9, Loss: 0.0013060752535238862\n",
            "Epoch 10, Loss: 0.0007990804733708501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ascii_art(model, tokenizer, device, max_length=1024, temperature=1.0):\n",
        "    input_ids = torch.tensor([[tokenizer.char_to_token[' ']]]).to(device)  # start with a space character\n",
        "    generated_art = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        outputs = model(input_ids)\n",
        "        next_token_logits = outputs[:, -1, :]\n",
        "        next_token_probs = F.softmax(next_token_logits / temperature, dim=-1)\n",
        "        next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
        "        generated_art.append(tokenizer.token_to_char[next_token.item()])\n",
        "        input_ids = torch.cat((input_ids, next_token), dim=1)\n",
        "\n",
        "    generated_art = ''.join(generated_art)\n",
        "    return generated_art\n",
        "    generated_art = generated_art.replace(' ', '')  # remove spaces\n",
        "    generated_art = generated_art.split('\\n')  # split into lines\n",
        "    max_width = max(len(line) for line in generated_art)\n",
        "    generated_art = [line.ljust(max_width) for line in generated_art]\n",
        "    generated_art = '\\n'.join(generated_art)\n",
        "    return generated_art\n",
        "\n",
        "generated_art = generate_ascii_art(model, tokenizer, device)\n",
        "print(len(generated_art))\n",
        "# check if all generated_art tokens are ' '\n",
        "if generated_art.count(' ') == len(generated_art):\n",
        "    print(\"All generated_art tokens are ' '\")\n",
        "else:\n",
        "    print(generated_art)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S2ZYsozv8Xj",
        "outputId": "8ba1f46f-251f-4739-c830-84d6a84d1c1f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n",
            "All generated_art tokens are ' '\n"
          ]
        }
      ]
    }
  ]
}